"""
Load node id and coordinates from pycgrc file generated by https://github.com/AndGem/OsmToRoadGraph to PostGIS database
Read list of coordinates and get nearest node id using index-based KNN https://postgis.net/workshops/postgis-intro/knn.html#index-based-knn

Example of config.ini:
```
[respondor]
host = localhost
user = postgres
passwd = postgres
db = respondor
```
@Author yohanes.gultom@gmail.com
"""

import configparser
import psycopg2
import sys
import csv
import json
import math
import multiprocessing as mp
import networkx as nx
import concurrent.futures
import matplotlib.pyplot as plt
from io import StringIO
from networkx.readwrite import json_graph
from itertools import combinations, permutations
from typing import Callable
from collections import OrderedDict 

config = configparser.ConfigParser()
config.read('config.ini')

class get_db_connection:
    """
    Simplify opening/closing db connection
    """
    def __init__(self, dbconf: dict):
        self.host = dbconf['host']
        self.database = dbconf['db']
        self.user = dbconf['user']
        self.password = dbconf['passwd']

    def __enter__(self):
        self.conn = psycopg2.connect(
            host=self.host,
            database=self.database,
            user=self.user,
            password=self.password)
        self.cur = self.conn.cursor()
        return self.cur

    def __exit__(self, type, value, traceback):
        self.conn.commit()
        self.cur.close()
        self.conn.close()

def create_subgraph(poi_input_path: str, graph_input_path: str, subgraph_output_path: str):
    """
    Read csv in poi_input_path containing point of interests (POIs) {name, lat, lon, id}
    Read json file in graph_input_path containing the complete graph (where the POIs are some of the nodes)
    Create subgraph for given POIs and save it as JSON in subgraph_output_path

    Args:
        poi_input_path (str): path of the csv containing POI having rows {name, lat, lon, id}
        graph_input_path (str): path to networkx json_graph.adjacency_data
        subgraph_output_path (str): path to write networkx subgraph as json_graph.adjacency_data
    """
    # read input
    node_ids = []
    print(f'reading POI file {poi_input_path}')
    with open(poi_input_path) as f:
        reader = csv.reader(f)
        for row in reader:
            assert len(row) >= 5
            node_id = row[4]
            node_ids.append(int(node_id))
    
    node_ids = list(set(node_ids))
    node_ids.sort()

    data = {}
    print(f'reading graph file {graph_input_path}') 
    with open(graph_input_path) as f:
        data = json.load(f)
    
    # create subgraph
    print(f'creating subgraph')
    G = json_graph.adjacency_graph(data)    
    len_node_ids = len(node_ids)
    # H = nx.to_undirected(G)    
    # num_combinations = math.factorial(len_node_ids) / (2*math.factorial((len_node_ids-2)))
    # print(f'* getting the shortest paths of {num_combinations} combinations')
    # futures = []
    # with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:
    #     for nodes in combinations(node_ids, r=2):
    #         futures.append(executor.submit(nx.shortest_path, H, *nodes))
    num_permutations = math.factorial(len_node_ids) / math.factorial((len_node_ids-2))
    print(f'* getting the shortest paths of {num_permutations} permutations')
    futures = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:
        for nodes in permutations(node_ids, r=2):
            futures.append(executor.submit(nx.shortest_path, G, *nodes))
    min_nodes = set()
    for future in futures:
        nodes = future.result()
        min_nodes.update(nodes)
    print('* create subgraph from the longest induced path')
    I = nx.subgraph(G, min_nodes)
    subgraph_data = json_graph.adjacency_data(I)
    with open(subgraph_output_path, 'w+') as f:
        json.dump(subgraph_data, f)
    print(f'subgraph saved in {subgraph_output_path}')
    
def graph_to_pycgr(graph_input_path: str, pycgr_output_path: str, node_id_map: dict = None):
    """
    Convert networkx.readwrite.json_graph.adjacency_data in graph_input_path
    to OsmToRoadGraph pycgr in pycgr_output_path

    Args:
        graph_input_path (str): path to networkx.readwrite.json_graph.adjacency_data
        pycgr_output_path (str): path to OsmToRoadGraph pycgr output
    """
    print(f'reading graph file {graph_input_path}') 
    with open(graph_input_path) as f:
        data = json.load(f)
    G = json_graph.adjacency_graph(data)
    bidirectional_count = len([1 for source_id, dest_id, data in G.edges.data() if G.has_edge(dest_id, source_id)])
    last_new_node_id = None
    if node_id_map:
        last_new_node_id = max(list(node_id_map.values()))
    with open(pycgr_output_path, 'w+') as f:
        f.write('# Road Graph File v.0.4\n')
        f.write('# number of nodes\n')
        f.write('# number of edges\n')
        f.write('# node_properties\n')
        f.write('# ...\n')
        f.write('# edge_properties\n')
        f.write('# ...\n')
        f.write(str(len(G.nodes)) + '\n')
        f.write(str(len(G.edges) - bidirectional_count // 2) + '\n')
        writer = csv.writer(f, delimiter=' ')
        for node_id, data in G.nodes.data():            
            if node_id_map:
                if node_id not in node_id_map:
                    last_new_node_id += 1
                    node_id_map[node_id] = last_new_node_id
                node_id = node_id_map[node_id]
            writer.writerow((node_id, data['lat'], data['lon']))
        bidirectionals = {}
        for source_id, dest_id, data in G.edges.data():
            if node_id_map:
                source_id = node_id_map[source_id]
                dest_id = node_id_map[dest_id]
            bidirectional = int(G.has_edge(dest_id, source_id))
            if bidirectional == 1:
                if f'{dest_id}-{source_id}' not in bidirectionals:
                    bidirectionals[f'{source_id}-{dest_id}'] = 1
                    writer.writerow((source_id, dest_id, data['length'], data['highway'], data['max_v'], bidirectional))
            else:
                 writer.writerow((source_id, dest_id, data['length'], data['highway'], data['max_v'], bidirectional))

def load_locations_and_routes(table_prefix: str, file_path: str):
    """
    Read locations and routes from pycgr/pycgrc file 
    generated from osm file by https://github.com/AndGem/OsmToRoadGraph
    and store them to PostgreSQL {table_prefix}_locations and {table_prefix}_routes

    Args:
        table_prefix (str): prefix for locations and routes tables
        file_path (str): pycgr/pycgrc file path
    Returns:
        count_nodes (int): number of locations stored
        count_edges (int): number of routes stored
    """
    # read nodes to string io
    s_locations = StringIO()
    s_routes = StringIO()
    total_nodes = None
    total_edges = None
    count_edges = 0
    count_nodes = 0
    print(f'reading {file_path}')
    with open(file_path) as f:
        count = 0
        for line in f:
            if count == 7:
                total_nodes = int(line)
            elif count == 8:
                total_edges = int(line)
            elif count > 8:
                if count_nodes < total_nodes:
                    # start reading nodes
                    node_id, lat, lon = line.split()
                    values = (node_id, f'POINT({lon} {lat})')
                    s_locations.write('\t'.join(values)+'\n')
                    count_nodes += 1            
                else:
                    # start reading edges
                    source_id, target_id, length, street_type, max_speed, bidirectional = line.split()
                    values = (source_id, target_id, length, street_type, max_speed, bidirectional)
                    s_routes.write('\t'.join(values)+'\n')
                    count_edges += 1
            count += 1
    s_locations.seek(0)        
    s_routes.seek(0)        

    assert count_nodes == total_nodes
    assert count_edges == total_edges

    # create table and copy data
    print(f'copying {count_nodes} locations and {count_edges} routes data to database')
    with get_db_connection(config['respondor']) as cur:
        # locations
        table_name_locations = f'{table_prefix}_locations'
        cur.execute(f'DROP TABLE IF EXISTS {table_name_locations}')
        cur.execute(f"""
        CREATE TABLE {table_name_locations} (
            id integer NOT NULL,
            coords geometry,
            CONSTRAINT {table_name_locations}_pkey PRIMARY KEY (id)
        )
        """
        )
        cur.execute(f'CREATE INDEX {table_name_locations}_coords_index ON {table_name_locations} USING GIST(coords)')
        cur.copy_from(s_locations, table_name_locations, columns=('id', 'coords'))

        # routes
        table_name_routes = f'{table_prefix}_routes'
        cur.execute(f'DROP TABLE IF EXISTS {table_name_routes}')
        cur.execute(f"""
        CREATE TABLE {table_name_routes} (
            id SERIAL PRIMARY KEY,
            source_id integer NOT NULL, 
            target_id integer NOT NULL,  
            length decimal(9,2) NOT NULL, 
            street_type varchar (255), 
            max_speed decimal(6,2), 
            bidirectional bool
        )
        """
        )
        cur.execute(f'CREATE INDEX idx_{table_name_routes}_source_id ON {table_name_routes}(source_id);')
        cur.execute(f'CREATE INDEX idx_{table_name_routes}_target_id ON {table_name_routes}(target_id);')
        cur.copy_from(s_routes, table_name_routes, columns=('source_id', 'target_id', 'length', 'street_type', 'max_speed', 'bidirectional'))
    
    print('completed')
    return count_nodes, count_edges

def get_nearest_node_ids(location_table_name: str, input_path: str):
    """
    Read csv in input_path containing {name, lat, lon} per rows
    find nearest location id in location_table_name {id, coords}
    rewrite the input file by adding the id so each rows become {name, lat, lon, id}

    Args:
        location_table_name (str): locations table's name
        input_path (str): path to csv with containing rows of {name, lat, lon}    
    """
    # read input
    locations = []
    print(f'reading file {input_path}')
    with open(input_path) as f:
        reader = csv.reader(f)
        for row in reader:
            assert len(row) >= 4
            name = row[0]
            cat = row[1]
            lat = row[2]
            lon = row[3]
            locations.append((name, cat, lat, lon))

    # find nearest location then rewrite input    
    with get_db_connection(config['respondor']) as cur, open(input_path, 'w+') as f:
        writer = csv.writer(f)
        for row in locations:
            name, cat, lat, lon = row
            sql = f"SELECT id FROM {location_table_name} ORDER BY coords <-> 'POINT({lon} {lat})'::geometry ASC LIMIT 1;"
            cur.execute(sql)
            res = cur.fetchone() 
            node_id = res[0]
            # print(f'({lat},{lon}) => {node_id}')
            writer.writerow((name, cat, lat, lon, node_id))

def resolve_conflicting_nodes(location_table_name: str, location_path: str, pycgr_path: str):
    """
    Make sure there is only one location pointing to one node for each category
    by creating a new node for each of other locations that point to the same node

    Args:
        location_table_name (str): locations table's name
        location_path (str): path to csv with containing rows of {name, cat, lat, lon, node_id}    
        pycgr_path (str): pycgr/pycgrc file path
    """

    # find max node_id
    max_id = None
    with get_db_connection(config['respondor']) as cur:
        sql = f"SELECT MAX(id) FROM {location_table_name};"
        cur.execute(sql)
        res = cur.fetchone() 
        max_id = res[0]
    print(max_id)
    assert max_id is not None
    
    # check if new nodes are required
    print('reading locations')
    locations = []
    node_key_map = {}
    new_node_map = OrderedDict()
    last_id = max_id
    with open(location_path) as f:
        reader = csv.reader(f)
        for row in reader:
            # assuming there is no real duplicate data in location_path,
            # create a new node for every other locations pointing to the same node
            name = row[0]
            cat = row[1]
            lat = row[2]
            lon = row[3]
            node_id = row[4]
            if node_id not in node_key_map:
                # mark node as assigned/used
                node_key_map[node_id] = 1
            else:
                # create new node
                last_id += 1
                new_node_map[str(last_id)] = node_id
                node_id = str(last_id)                    
            locations.append((name, cat, lat, lon, node_id))

    if not new_node_map:
        print('no new nodes')
        return

    # read nodes and edges
    nodes = OrderedDict()
    edges = []
    total_nodes = None
    total_edges = None
    count_edges = 0
    count_nodes = 0
    print(f'reading pycgr')
    with open(pycgr_path) as f:
        count = 0
        for line in f:
            if count == 7:
                total_nodes = int(line)
            elif count == 8:
                total_edges = int(line)
            elif count > 8:
                if count_nodes < total_nodes:
                    # start reading nodes
                    node_id, lat, lon = line.split()
                    nodes[node_id] = {
                        'lat': lat,
                        'lon': lon,
                        'edges': [],
                    }                     
                    count_nodes += 1            
                else:
                    # start reading edges
                    values = line.split()                    
                    source_id = values[0]
                    dest_id = values[1]
                    nodes[source_id]['edges'].append(values)
                    nodes[dest_id]['edges'].append(values)
                    edges.append(values)
                    count_edges += 1
            count += 1

    # append new nodes
    total_new_nodes = 0
    total_new_edges = 0
    for new_node_id, ori_node_id in new_node_map.items():
        if new_node_id not in nodes:
            ori_node = nodes[ori_node_id]
            nodes[new_node_id] = {
                'lat': ori_node['lat'],
                'lon': ori_node['lon'],
            }

            # copy edges, replacing source_id or target_id
            for source_id, target_id, length, street_type, max_speed, bidirectional in ori_node['edges']:
                if source_id == ori_node_id:
                    source_id = new_node_id
                elif target_id == ori_node_id:
                    target_id = new_node_id
                else:
                    raise RuntimeError(f'ori_node_id {ori_node_id} does not match source_id {source_id} or target_id {target_id}')
                copy_edge = [source_id, target_id, length, street_type, max_speed, bidirectional]
                edges.append(copy_edge)
                total_new_edges += 1

            # add one edge to connect new_node to ori_node
            length = str(10.0) # 10 meters
            street_type = 'unclassified'
            max_speed = str(10) # 10 km/h
            bidirectional = str(1) # bidirectional
            new_edge = [new_node_id, ori_node_id, length, street_type, max_speed, bidirectional]
            edges.append(new_edge)
            total_new_edges += 1
            total_new_nodes += 1

    assert len(new_node_map) == total_new_nodes

    # rewrite pycgr appending new nodes and edges
    print('rewriting pycgr')    
    with open(pycgr_path, 'w+') as f:        
        f.write('# Road Graph File v.0.4\n')
        f.write('# number of nodes\n')
        f.write('# number of edges\n')
        f.write('# node_properties\n')
        f.write('# ...\n')
        f.write('# edge_properties\n')
        f.write('# ...\n')
        # make sure starts after the 7th line
        f.write(str(total_nodes + total_new_nodes) + '\n')
        f.write(str(total_edges + total_new_edges) + '\n')
        # nodes        
        writer = csv.writer(f, delimiter=' ')                        
        for node_id, data in nodes.items():
            writer.writerow((node_id, data['lat'], data['lon']))
        # edges
        for edge in edges:
            writer.writerow(edge)

    # rewriting locations
    print(f'rewriting locations')
    with open(location_path, 'w+') as f:
        writer = csv.writer(f)
        for row in locations:            
            writer.writerow(row)

            
def reindex_graph_locations(location_input_path: str):
    """
    Reindex location_input_path based on location type
    Append new index as new column in location_input_path

    Args:
        location_input_path (str): path to csv with containing rows of {name, lat, lon, node_id}    
    """
    # group node id by category
    print('reading and grouping node ids by category')
    keys = ['village', 'shelter', 'depot', 'other']
    node_id_by_cat = {k: [] for k in keys}
    locations = []
    with open(location_input_path) as f:
        reader = csv.reader(f)
        for row in reader:
            assert len(row) >= 5
            name = row[0].strip()
            cat = row[1].lower().strip()
            lat = row[2]
            lon = row[3]
            node_id = int(row[4])
            locations.append((name, cat, lat, lon, node_id))
            if cat in ['village', 'shelter', 'depot']:
                node_id_by_cat[cat.lower()].append(node_id)
            else:
                node_id_by_cat['other'].append(node_id)

    # create map to new index
    print('creating map to new index')    
    node_id_map = {}
    count_map = {k: 1 for k in keys}
    for key in keys:
        node_id_by_cat[key].sort()
        for node_id in node_id_by_cat[key]:
            node_id_map[node_id] = count_map[key]
            count_map[key] += 1
        
    # append new node id to location file
    print(f'adding new node index to location {location_input_path}')
    with open(location_input_path, 'w+') as f:
        writer = csv.writer(f)
        for row in locations:
            name = row[0]
            cat = row[1]
            lat = row[2]
            lon = row[3]
            node_id = row[4]
            new_node_id = node_id_map[node_id]
            writer.writerow((name, cat, lat, lon, node_id, new_node_id))